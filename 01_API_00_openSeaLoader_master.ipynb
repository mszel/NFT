{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API: Downloading and Testing data\n",
    "Downloading the event, asset and collection information by using OpenSea's API.\n",
    " * Downloading data\n",
    " * Run tests and mark risks (missing or duplicated data): e.g. checking the delta trx time between events, and mark if it is too big or unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing related\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# core libraries\n",
    "from datetime import datetime, timedelta\n",
    "import imp\n",
    "\n",
    "# OS related\n",
    "from os import listdir, makedirs, remove, path\n",
    "\n",
    "# parallel programming related\n",
    "from multiprocessing import Pool\n",
    "import subprocess\n",
    "\n",
    "# downloading related\n",
    "import requests\n",
    "\n",
    "# OpenSea related\n",
    "\n",
    "# settings\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# own libraries\n",
    "import src.config_API as ca\n",
    "import src.api_dl as dl\n",
    "import src.config as cf\n",
    "import src.util as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = imp.reload(ca)\n",
    "cf = imp.reload(cf)\n",
    "dl = imp.reload(dl)\n",
    "ut = imp.reload(ut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function test: single thread download\n",
    "Downloading/storing transactions within a time interval (with the given filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a few hours (single thread loader)\n",
    "\n",
    "# dl._timeInterval_eventDL_oneThread(API_key=ca.apikey_1, filter_dict={'event_type':'successful'}, \n",
    "#                                    time_interval=['2021-01-02 00:00:00', '2021-01-03 00:00:00'], \n",
    "#                                    path_dumpdir=cf.PATH_00RAW_API_dump_main + 'test/', batchsize=300, \n",
    "#                                    _verbose=True, _timeout_limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_results = pd.read_pickle(cf.PATH_00RAW_API_dump_main + 'test/eventresponse_1609545600_1609632000.pickle')\n",
    "# pdf_results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sun 15:00:06] Downloading events from 1531526400 to 1531612800...\n",
      "[Sun 15:00:06] File already exists...\n"
     ]
    }
   ],
   "source": [
    "dl = imp.reload(dl) # 1610064000\n",
    "dl._timeInterval_eventDL_oneThread(API_key=ca.apikey_1, filter_dict={'event_type':'successful'}, \n",
    "                                   time_interval=[], time_interval_unixts=[1531526400, 1531612800],\n",
    "                                   path_dumpdir=cf.PATH_00RAW_API_dump_main + 'test/', batchsize=300, \n",
    "                                   _verbose=True, _timeout_limit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Earlier Years and Recent Data\n",
    "At the beginning, it is possible to load bigger periods (although, in 2017, there are some \"busy days\" - probably the initial transactions/NFTs were uploaded). Later on, shorther periods can be used: days (time_batch_sec=86400), half days (time_batch_sec=43200), or hours (time_batch_sec=3600). \n",
    "If you get too many errors in the errors folder (some days are not available / some periods give you error), two things can be tried:\n",
    " * (0. check the errors, it might answer what is the problem)\n",
    " * Try to download the data a few days/week later. OpenSea will fix the error if it is on their side.\n",
    " * Use lower time_batch_sec (5-minute = 300 sec for instance), so the \"in-between\" gaps will be filled at least. The API loader automatically handling the covered time intervals, independently from the time_batch_sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are downloading the data for 2017 (not full year), 2018, 2019, 2020, 2021 and the recent period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier Years (weekly dl)\n",
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2017-01-01 00:00:00', '2018-01-01 00:00:00'], time_batch_sec=1*86400, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier Years (weekly dl)\n",
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2018-01-01 00:00:00', '2019-01-01 00:00:00'], time_batch_sec=86400, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2019-01-01 00:00:00', '2020-01-01 00:00:00'], time_batch_sec=86400, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a month (multi-thread loader)\n",
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2020-01-01 00:00:00', '2021-01-01 00:00:00'], time_batch_sec=86400, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a month (multi-thread loader)\n",
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2021-01-01 00:00:00', '2021-04-01 00:00:00'], time_batch_sec=43200, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2021-04-01 00:00:00', '2021-07-01 00:00:00'], time_batch_sec=43200, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = imp.reload(dl)\n",
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2021-07-01 00:00:00', '2021-10-01 00:00:00'], time_batch_sec=43200, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "                       time_interval=['2021-10-01 00:00:00', '2022-01-01 00:00:00'], time_batch_sec=21600, \n",
    "                       path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, \n",
    "                       _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "#                        time_interval=['2021-05-01 00:00:00', '2021-06-01 00:00:00'], time_batch_sec=43200, \n",
    "#                        path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "#                        time_interval=['2021-06-01 00:00:00', '2021-07-01 00:00:00'], time_batch_sec=86400, \n",
    "#                        path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, _verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.TimeIntervalEventDL(list_of_API_keys=[ca.apikey_1, ca.apikey_2], filter_dict={'event_type':'successful'}, \n",
    "#                        time_interval=['2021-07-01 00:00:00', '2021-08-01 00:00:00'], time_batch_sec=86400, \n",
    "#                        path_out_dumpdir=cf.PATH_00RAW_API_dump_main, batchsize=300, _timeout_limit=0, _verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare some Tests\n",
    "Testing the downloaded transactions for filtering out possible errors: \n",
    " * The first/last transactions near to the \"cut\" (for all files downloaded). Compare the differences to the AVG distance cross transactions inside the file.\n",
    " * The daily amount of transactions (Compare it to the 30-day moving average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the distance (in sec) from the file limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a library\n",
    "\n",
    "def _libTester_event(path_in_dir):\n",
    "    \"\"\"\n",
    "    Testing the event dump files in a library, for identifying the potential errors.\n",
    "    \"\"\"\n",
    "\n",
    "    _list_of_files = listdir(path_in_dir)\n",
    "    _list_of_files = [_f for _f in _list_of_files if (_f.count('.pickle') > 0)]\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for _file in _list_of_files:\n",
    "        _mindt = int(_file.split('.')[0].split('_')[1])\n",
    "        _maxdt = int(_file.split('.')[0].split('_')[2])\n",
    "        _tmp_df = pd.read_pickle(path_in_dir + _file)\n",
    "        _realmindt = pd.to_datetime(_tmp_df.iloc[-1].transaction['timestamp']).timestamp()\n",
    "        _realmaxdt = pd.to_datetime(_tmp_df.iloc[0].transaction['timestamp']).timestamp()\n",
    "        _odf = pd.DataFrame({'expected_min_dt':[_mindt], 'real_min_dt':[_realmindt], \n",
    "                             'expected_max_dt':[_maxdt], 'real_max_dt':[_realmaxdt]})\n",
    "        out_df = out_df.append(_odf)\n",
    "    \n",
    "    out_df = out_df.reset_index(drop=True)\n",
    "    out_df['min_dt_diff'] = out_df.real_min_dt - out_df.expected_min_dt\n",
    "    out_df['max_dt_diff'] = out_df.expected_max_dt - out_df.real_max_dt\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _libTester_event('./dump_data/events_20210401_000000_20210501_000000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a6613bf63225e9394175f1e2a262c6dd4ee2ff1bd210ad1657e6dcd2c752f3b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
